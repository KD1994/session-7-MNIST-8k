{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# IMPORTS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "0m2JWFliFfKT"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from __future__ import print_function\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torchinfo import summary\n",
        "from tqdm import tqdm\n",
        "from model import MnistNet_3\n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# MODEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "xdydjYTZFyi3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "==========================================================================================\n",
              "Layer (type:depth-idx)                   Output Shape              Param #\n",
              "==========================================================================================\n",
              "MnistNet_3                               [1, 10]                   --\n",
              "├─Sequential: 1-1                        [1, 8, 12, 12]            --\n",
              "│    └─Conv2d: 2-1                       [1, 8, 26, 26]            72\n",
              "│    └─BatchNorm2d: 2-2                  [1, 8, 26, 26]            16\n",
              "│    └─ReLU: 2-3                         [1, 8, 26, 26]            --\n",
              "│    └─Conv2d: 2-4                       [1, 16, 24, 24]           1,152\n",
              "│    └─BatchNorm2d: 2-5                  [1, 16, 24, 24]           32\n",
              "│    └─ReLU: 2-6                         [1, 16, 24, 24]           --\n",
              "│    └─Conv2d: 2-7                       [1, 8, 24, 24]            128\n",
              "│    └─MaxPool2d: 2-8                    [1, 8, 12, 12]            --\n",
              "├─Sequential: 1-2                        [1, 16, 6, 6]             --\n",
              "│    └─Conv2d: 2-9                       [1, 12, 10, 10]           864\n",
              "│    └─BatchNorm2d: 2-10                 [1, 12, 10, 10]           24\n",
              "│    └─Dropout: 2-11                     [1, 12, 10, 10]           --\n",
              "│    └─ReLU: 2-12                        [1, 12, 10, 10]           --\n",
              "│    └─Conv2d: 2-13                      [1, 16, 8, 8]             1,728\n",
              "│    └─BatchNorm2d: 2-14                 [1, 16, 8, 8]             32\n",
              "│    └─ReLU: 2-15                        [1, 16, 8, 8]             --\n",
              "│    └─Conv2d: 2-16                      [1, 16, 6, 6]             2,304\n",
              "│    └─BatchNorm2d: 2-17                 [1, 16, 6, 6]             32\n",
              "│    └─ReLU: 2-18                        [1, 16, 6, 6]             --\n",
              "├─AdaptiveAvgPool2d: 1-3                 [1, 16, 1, 1]             --\n",
              "├─Sequential: 1-4                        [1, 10, 1, 1]             --\n",
              "│    └─Conv2d: 2-19                      [1, 10, 1, 1]             160\n",
              "==========================================================================================\n",
              "Total params: 6,544\n",
              "Trainable params: 6,544\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (M): 1.07\n",
              "==========================================================================================\n",
              "Input size (MB): 0.00\n",
              "Forward/backward pass size (MB): 0.32\n",
              "Params size (MB): 0.03\n",
              "Estimated Total Size (MB): 0.35\n",
              "=========================================================================================="
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = MnistNet_3()\n",
        "summary(model, input_size=(1, 1, 28, 28))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# DATALOADER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "DqTWLaM5GHgH"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(1)\n",
        "if use_cuda:\n",
        "    torch.cuda.manual_seed(1)\n",
        "\n",
        "batch_size = 128\n",
        "\n",
        "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('data', train=True, download=True,\n",
        "                    transform=transforms.Compose([\n",
        "                        transforms.RandomRotation((-7.0, 7.0), fill=(1,)),\n",
        "                        transforms.RandomAffine(degrees=10,  scale=(0.95, 1.05)),\n",
        "                        # transforms.ColorJitter(brightness=0.10, contrast=0.1, saturation=0.10, hue=0.1),\n",
        "                        transforms.ToTensor(),\n",
        "                        transforms.Normalize((0.1307,), (0.3081,))\n",
        "                    ])),\n",
        "                    batch_size=batch_size, shuffle=True, **kwargs)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('data', train=False, transform=transforms.Compose([\n",
        "                        transforms.ToTensor(),\n",
        "                        transforms.Normalize((0.1307,), (0.3081,))\n",
        "                    ])),\n",
        "                    batch_size=batch_size, shuffle=True, **kwargs)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# TRAINING & TESTING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "8fDefDhaFlwH"
      },
      "outputs": [],
      "source": [
        "def train(model, device, train_loader, optimizer, criterion, epoch):\n",
        "    model.train()\n",
        "    pbar = tqdm(train_loader)\n",
        "    correct = 0\n",
        "    for batch_idx, (data, target) in enumerate(pbar):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        pbar.set_description(desc= f'Epoch={epoch} Batch={batch_idx} loss={loss.item():.7f} Accuracy={100. * correct / len(train_loader.dataset):.2f}%')\n",
        "\n",
        "\n",
        "def test(model, device, test_loader, criterion):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += criterion(output, target).item()  # sum up batch loss\n",
        "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.7f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# RUN-1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "MMWbLWO6FuHb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Adjusting learning rate of group 0 to 1.5000e-02.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch=0 Batch=468 loss=0.0943284 Accuracy=82.46%: 100%|██████████| 469/469 [00:10<00:00, 44.77it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0007256, Accuracy: 9764/10000 (97.64%)\n",
            "\n",
            "Adjusting learning rate of group 0 to 1.5000e-02.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch=1 Batch=468 loss=0.1027418 Accuracy=97.04%: 100%|██████████| 469/469 [00:10<00:00, 44.11it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0005218, Accuracy: 9803/10000 (98.03%)\n",
            "\n",
            "Adjusting learning rate of group 0 to 1.5000e-02.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch=2 Batch=468 loss=0.1157867 Accuracy=97.77%: 100%|██████████| 469/469 [00:10<00:00, 45.08it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0003684, Accuracy: 9866/10000 (98.66%)\n",
            "\n",
            "Adjusting learning rate of group 0 to 1.5000e-02.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch=3 Batch=468 loss=0.1263060 Accuracy=98.05%: 100%|██████████| 469/469 [00:10<00:00, 43.76it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0003454, Accuracy: 9867/10000 (98.67%)\n",
            "\n",
            "Adjusting learning rate of group 0 to 9.0000e-03.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch=4 Batch=468 loss=0.0220200 Accuracy=98.45%: 100%|██████████| 469/469 [00:10<00:00, 44.77it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0002263, Accuracy: 9914/10000 (99.14%)\n",
            "\n",
            "Adjusting learning rate of group 0 to 9.0000e-03.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch=5 Batch=468 loss=0.0843462 Accuracy=98.57%: 100%|██████████| 469/469 [00:10<00:00, 43.87it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0002303, Accuracy: 9907/10000 (99.07%)\n",
            "\n",
            "Adjusting learning rate of group 0 to 9.0000e-03.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch=6 Batch=468 loss=0.0377104 Accuracy=98.59%: 100%|██████████| 469/469 [00:10<00:00, 44.66it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0002262, Accuracy: 9918/10000 (99.18%)\n",
            "\n",
            "Adjusting learning rate of group 0 to 9.0000e-03.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch=7 Batch=468 loss=0.0344428 Accuracy=98.62%: 100%|██████████| 469/469 [00:10<00:00, 42.75it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0001873, Accuracy: 9924/10000 (99.24%)\n",
            "\n",
            "Adjusting learning rate of group 0 to 5.4000e-03.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch=8 Batch=468 loss=0.0607714 Accuracy=98.76%: 100%|██████████| 469/469 [00:10<00:00, 44.90it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0001683, Accuracy: 9924/10000 (99.24%)\n",
            "\n",
            "Adjusting learning rate of group 0 to 5.4000e-03.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch=9 Batch=468 loss=0.0753321 Accuracy=98.78%: 100%|██████████| 469/469 [00:10<00:00, 45.66it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0001699, Accuracy: 9935/10000 (99.35%)\n",
            "\n",
            "Adjusting learning rate of group 0 to 5.4000e-03.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch=10 Batch=468 loss=0.0294918 Accuracy=98.82%: 100%|██████████| 469/469 [00:10<00:00, 43.68it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0001819, Accuracy: 9929/10000 (99.29%)\n",
            "\n",
            "Adjusting learning rate of group 0 to 5.4000e-03.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch=11 Batch=468 loss=0.0242449 Accuracy=98.89%: 100%|██████████| 469/469 [00:10<00:00, 44.30it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0001753, Accuracy: 9935/10000 (99.35%)\n",
            "\n",
            "Adjusting learning rate of group 0 to 3.2400e-03.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch=12 Batch=468 loss=0.0360408 Accuracy=98.87%: 100%|██████████| 469/469 [00:10<00:00, 43.75it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0001573, Accuracy: 9940/10000 (99.40%)\n",
            "\n",
            "Adjusting learning rate of group 0 to 3.2400e-03.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch=13 Batch=468 loss=0.0479270 Accuracy=98.96%: 100%|██████████| 469/469 [00:10<00:00, 44.38it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0001539, Accuracy: 9937/10000 (99.37%)\n",
            "\n",
            "Adjusting learning rate of group 0 to 3.2400e-03.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch=14 Batch=468 loss=0.0160867 Accuracy=98.95%: 100%|██████████| 469/469 [00:10<00:00, 45.27it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0001520, Accuracy: 9940/10000 (99.40%)\n",
            "\n",
            "Adjusting learning rate of group 0 to 3.2400e-03.\n"
          ]
        }
      ],
      "source": [
        "model = MnistNet_3().to(device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.015, momentum=0.9)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=4, gamma=0.6, verbose=True)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "for epoch in range(15):\n",
        "    train(model, device, train_loader, optimizer, criterion, epoch)\n",
        "    test(model, device, test_loader, criterion)\n",
        "    scheduler.step()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Target\n",
        "- Try to reduce parameters to near 8K\n",
        "- Try adding batch normalization and regularization\n",
        "- Try above 2 at the same time\n",
        "\n",
        "# Result\n",
        "- RUN-1 with just 7,416 parameters without batch normalization and regularization:\n",
        "    - Parameters: 7,416\n",
        "    - Best Training Accuracy: 98.91%\n",
        "    - Best Test Accuracy: 98.59%\n",
        "- RUN-2 with 7,416 parameters with batch normalization and regularization:\n",
        "    - Parameters: 7,416\n",
        "    - Best Training Accuracy: 99.23%\n",
        "    - Best Test Accuracy: 99.14%\n",
        "\n",
        "# Analysis\n",
        "- Train-test gap is as minimal as possible\n",
        "- able to reach 99.14% test accuracy, potential to reach 99.4% at least after adding image augmentation and GAP"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
